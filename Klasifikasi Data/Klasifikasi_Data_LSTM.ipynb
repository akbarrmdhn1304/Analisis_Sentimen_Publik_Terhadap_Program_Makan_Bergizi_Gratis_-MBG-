{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Klasifikasi Data\n"
      ],
      "metadata": {
        "id": "Al-70wjHgcwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eksperimen 4"
      ],
      "metadata": {
        "id": "oFcIQQvzghwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5P6kxboYggz_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data_bersih_filter_manual.csv')"
      ],
      "metadata": {
        "id": "TJf72cxhg0WD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Filter Data"
      ],
      "metadata": {
        "id": "4bv6EVuwhHvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Filter Data Labeled\n",
        "# Kita membuang baris yang kolom 'label_manual'-nya kosong (NaN)\n",
        "df_labeled = df.dropna(subset=['label_manual']).copy()\n",
        "\n",
        "# Opsional: Mengubah format label dari float (1.0) menjadi integer (1) agar lebih rapi\n",
        "df_labeled['label_manual'] = df_labeled['label_manual'].astype(int)\n",
        "\n",
        "print(f\"Total data setelah filter (hanya yang berlabel): {len(df_labeled)}\")\n",
        "print(\"Sebaran label:\\n\", df_labeled['label_manual'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVIt8tEhGqW",
        "outputId": "ae00cf74-bf71-425e-b292-bc1a17b3d54c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data setelah filter (hanya yang berlabel): 267\n",
            "Sebaran label:\n",
            " label_manual\n",
            " 1    103\n",
            "-1    101\n",
            " 0     63\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "FJAg1rLnhJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Proses Stemming dengan Sastrawi\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Fungsi untuk stemming satu kalimat\n",
        "def stemming_text(text):\n",
        "    if pd.isna(text): # Cek jika text kosong\n",
        "        return \"\"\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "# Terapkan ke kolom 'text_clean'\n",
        "# Kita buat kolom baru 'text_stemmed' agar data asli tidak hilang\n",
        "df_labeled['text_stemmed'] = df_labeled['text_clean'].apply(stemming_text)"
      ],
      "metadata": {
        "id": "0wedsFq8hI9u"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Lihat Hasil\n",
        "print(\"\\nContoh hasil stemming:\")\n",
        "print(df_labeled[['text_clean', 'text_stemmed']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWoRlGNehKib",
        "outputId": "ac56d2f8-c359-4d20-bb34-10448b38176d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh hasil stemming:\n",
            "                                          text_clean  \\\n",
            "1  bubar aja makan bergizi gratis itu bagus di ka...   \n",
            "2  wajib diapresiasi presiden prabowo konsisten p...   \n",
            "3                                     astaghfirullah   \n",
            "6  saya aman makan bergizi gratis gratis wajah te...   \n",
            "7  sdn randuagung utara dapat membagi banyak yang...   \n",
            "\n",
            "                                        text_stemmed  \n",
            "1  bubar aja makan gizi gratis itu bagus di kasih...  \n",
            "2  wajib apresiasi presiden prabowo konsisten pri...  \n",
            "3                                     astaghfirullah  \n",
            "6  saya aman makan gizi gratis gratis wajah senyu...  \n",
            "7   sdn randuagung utara dapat bagi banyak yang enak  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mapping Label (-1 -> 0, 0 -> 1, 1 -> 2)\n",
        "label_map = {-1: 0, 0: 1, 1: 2}\n",
        "df_labeled['label_final'] = df_labeled['label_manual'].map(label_map)"
      ],
      "metadata": {
        "id": "xZoH7Nw9jDe_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenisasi (Memecah kalimat menjadi list kata)\n",
        "# Kita gunakan kolom 'text_stemmed' yang sudah Anda buat\n",
        "texts = df_labeled['text_stemmed'].astype(str).tolist()\n",
        "labels = df_labeled['label_final'].tolist()"
      ],
      "metadata": {
        "id": "rLm12uY6jD6e"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split kalimat menjadi kata-kata (token)\n",
        "tokenized_texts = [text.split() for text in texts]"
      ],
      "metadata": {
        "id": "Cc1qyNdKjFQP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec"
      ],
      "metadata": {
        "id": "nLZjG1s5jope"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Latih Word2Vec\n",
        "# Kita latih dengan window=5 (konteks 5 kata) dan vector_size=100\n",
        "print(\"Melatih Word2Vec...\")\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Word2Vec selesai dilatih.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4YR2GOwjGp-",
        "outputId": "b109af75-12ec-438b-9052-def96b6cb23c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melatih Word2Vec...\n",
            "Word2Vec selesai dilatih.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Buat Vocabulary & Embedding Matrix\n",
        "# Ini adalah kamus untuk mengubah kata -> indeks angka\n",
        "word2idx = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)} # 0 disediakan untuk padding\n",
        "vocab_size = len(word2idx) + 1\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "Ge6vQ9YCjIGe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "xQeLQxhGjtTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat matriks bobot untuk layer Embedding LSTM nanti\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "embedding_tensor = torch.FloatTensor(embedding_matrix)\n",
        "print(f\"Ukuran Vocabulary: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAzANj53jJiW",
        "outputId": "03ea821a-e0be-44f4-b0a9-59be8a341d0b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ukuran Vocabulary: 1346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. PADDING & SPLIT DATA\n",
        "# ==========================================\n",
        "\n",
        "# Tentukan panjang maksimal kalimat (misal 50 kata)\n",
        "max_len = 50\n",
        "\n",
        "def encode_and_pad(tokens):\n",
        "    # Ubah kata jadi angka\n",
        "    encoded = [word2idx.get(t, 0) for t in tokens]\n",
        "\n",
        "    # Padding atau Potong\n",
        "    if len(encoded) < max_len:\n",
        "        encoded += [0] * (max_len - len(encoded)) # Tambah 0 di belakang\n",
        "    else:\n",
        "        encoded = encoded[:max_len] # Potong jika kepanjangan\n",
        "    return encoded\n",
        "\n",
        "# Terapkan ke semua data\n",
        "X_encoded = [encode_and_pad(t) for t in tokenized_texts]"
      ],
      "metadata": {
        "id": "hk17c6BXjLRW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubah ke format Tensor PyTorch\n",
        "X_tensor = torch.tensor(X_encoded, dtype=torch.long)\n",
        "y_tensor = torch.tensor(labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "4y-F9yNZjRt3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Train (80%) dan Test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor\n",
        ")"
      ],
      "metadata": {
        "id": "oHFcboHejUPu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat DataLoader\n",
        "train_data = list(zip(X_train, y_train))\n",
        "test_data = list(zip(X_test, y_test))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"Jumlah Data Train: {len(X_train)}\")\n",
        "print(f\"Jumlah Data Test: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8UMf8L8jVdm",
        "outputId": "ce2d6f16-1377-41af-9db7-d8fb87c7903f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah Data Train: 213\n",
            "Jumlah Data Test: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. DEFINISI MODEL LSTM\n",
        "# ==========================================\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, embedding_matrix):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        # Layer Embedding (menggunakan bobot dari Word2Vec)\n",
        "        # freeze=False artinya kita izinkan model mengupdate bobot kata-katanya lagi\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "\n",
        "        # Layer LSTM\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Layer Output (Fully Connected)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Masuk embedding\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Masuk LSTM\n",
        "        # output: (batch, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch, hidden_dim)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "        # Kita ambil hidden state terakhir (hidden[-1])\n",
        "        return self.fc(hidden[-1])"
      ],
      "metadata": {
        "id": "DSyUb78njY0e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi Model\n",
        "model = SentimentLSTM(vocab_size=vocab_size,\n",
        "                      embed_dim=embedding_dim,\n",
        "                      hidden_dim=64,   # Bisa diganti 128 jika kurang kompleks\n",
        "                      output_dim=3,    # 3 Kelas: Negatif, Netral, Positif\n",
        "                      embedding_matrix=embedding_tensor)"
      ],
      "metadata": {
        "id": "H62XIDGejZZm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek device (gunakan GPU jika ada)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Menggunakan device: {device}\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrN5YLgbja5n",
        "outputId": "ae39bc80-64f5-4138-e557-b696e8ae5f95"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menggunakan device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. TRAINING MODEL\n",
        "# ==========================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20 # Coba 15-20 epoch untuk LSTM\n",
        "print(\"Mulai Training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training Selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvSGatjXjfE2",
        "outputId": "36aa16b0-0fc6-4d30-f717-a434760ae830"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mulai Training...\n",
            "Epoch 5/20, Loss: 1.0675\n",
            "Epoch 10/20, Loss: 1.0228\n",
            "Epoch 15/20, Loss: 0.9793\n",
            "Epoch 20/20, Loss: 0.8593\n",
            "Training Selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 9. EVALUASI\n",
        "# ==========================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "# Hitung Akurasi\n",
        "acc = accuracy_score(all_targets, all_preds)\n",
        "print(f\"\\nAkurasi LSTM + Word2Vec: {acc:.2%}\")\n",
        "\n",
        "# Tampilkan Laporan Klasifikasi\n",
        "target_names = ['Negatif', 'Netral', 'Positif'] # Sesuai urutan 0, 1, 2\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_targets, all_preds, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYayzrwVjk0W",
        "outputId": "c9689b60-2337-41ba-e6a0-f17e3dba5a21"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi LSTM + Word2Vec: 42.59%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.42      0.80      0.55        20\n",
            "      Netral       0.43      0.46      0.44        13\n",
            "     Positif       0.50      0.05      0.09        21\n",
            "\n",
            "    accuracy                           0.43        54\n",
            "   macro avg       0.45      0.44      0.36        54\n",
            "weighted avg       0.45      0.43      0.35        54\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}